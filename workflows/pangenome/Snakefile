import os
import glob
from pathlib import Path
import sys

# Load config
configfile: "config.yaml"

# Set working directory relative to Snakefile
WORKFLOW_DIR = Path(workflow.basedir)
PROJECT_ROOT = WORKFLOW_DIR.parent.parent

# Resolve paths
REFERENCE = Path(config["reference"]).resolve()
# Updated: Use haplotype_data_dir from config
HAPLOTYPE_DATA_DIR = Path(config["haplotype_data_dir"]).resolve()
OUTPUT_DIR = Path(config["output_dir"]).resolve()

# Create output directory
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Docker image
VG_DOCKER = "quay.io/vgteam/vg:v1.65.0"

# Get current user ID and group ID for Docker
USER_ID = os.getuid()
GROUP_ID = os.getgid()

# Find all haplotype FASTA files
# Updated: Look for sample_XXX_hap1.fasta and sample_XXX_hap2.fasta
HAPLOTYPE_FASTAS = sorted(glob.glob(str(HAPLOTYPE_DATA_DIR / "sample_*" / "sample_*.fasta")))

# Create a list of unique sample names (e.g., "sample_000")
UNIQUE_SAMPLE_NAMES = sorted(list(set([Path(f).parent.name for f in HAPLOTYPE_FASTAS])))

# Create a list of paths for the graph (e.g., "sample_000_hap1", "sample_000_hap2")
GRAPH_PATH_NAMES = []
for sample_name in UNIQUE_SAMPLE_NAMES:
    GRAPH_PATH_NAMES.append(f"{sample_name}_hap1")
    GRAPH_PATH_NAMES.append(f"{sample_name}_hap2")

print(f"Found {len(HAPLOTYPE_FASTAS)} haplotype FASTA files for {len(UNIQUE_SAMPLE_NAMES)} samples")
print(f"Graph paths will be: {GRAPH_PATH_NAMES}")

if not HAPLOTYPE_FASTAS:
    print("ERROR: No haplotype FASTA files found!")
    print(f"Looking in: {HAPLOTYPE_DATA_DIR}")
    print(f"Pattern: sample_*/sample_*.fasta")
    sys.exit(1)

# Final outputs
rule all:
    input:
        OUTPUT_DIR / "pangenome.gfa",
        OUTPUT_DIR / "pangenome_stats.txt",
        OUTPUT_DIR / "sample_alignments.paf"

# Create combined FASTA with all sequences
rule combine_sequences:
    input:
        ref=REFERENCE,
        haplotypes=HAPLOTYPE_FASTAS
    output:
        combined=OUTPUT_DIR / "all_sequences.fa"
    log:
        OUTPUT_DIR / "logs" / "combine_sequences.log"
    run:
        with open(str(output.combined), 'w') as out:
            # Add reference with clear naming
            print(f"Adding reference: {input.ref}", file=open(str(log), 'w'))
            with open(str(input.ref)) as f:
                for line in f:
                    if line.startswith('>'):
                        out.write('>reference\n') # Ensure reference sequence is named 'reference'
                    else:
                        out.write(line)
            
            # Add each haplotype
            # Sort haplotypes to ensure consistent ordering (hap1 before hap2)
            sorted_haplotypes = sorted(input.haplotypes)
            for var_path in sorted_haplotypes:
                # Extract sample_XXX_hapX from path
                haplotype_name = Path(var_path).stem 
                print(f"Adding haplotype {haplotype_name}: {var_path}", file=open(str(log), 'a'))
                with open(var_path) as f:
                    for line in f:
                        if line.startswith('>'):
                            out.write(f'>{haplotype_name}\n') # Name sequence by its haplotype name
                        else:
                            out.write(line)

# Create all-vs-all alignment (no Docker needed)
rule align_all:
    input:
        seqs=OUTPUT_DIR / "all_sequences.fa"
    output:
        paf=OUTPUT_DIR / "sample_alignments.paf"
    threads: config.get("threads", 4)
    log:
        OUTPUT_DIR / "logs" / "align_all.log"
    shell:
        """
        minimap2 -x asm20 -X -c -t {threads} {input.seqs} {input.seqs} > {output.paf} 2> {log}
        echo "Alignment complete. Lines in PAF:" >> {log}
        wc -l {output.paf} >> {log}
        """

# Build pangenome using vg with Docker
rule build_pangenome:
    input:
        seqs=OUTPUT_DIR / "all_sequences.fa"
    output:
        vg=OUTPUT_DIR / "pangenome.vg",
        gfa=OUTPUT_DIR / "pangenome.gfa"
    threads: config.get("threads", 4)
    log:
        OUTPUT_DIR / "logs" / "build_pangenome.log"
    params:
        # Get parent directory for mounting
        mount_dir=str(OUTPUT_DIR.parent.parent.absolute())
    shell:
        """
        # Build graph using msga with Docker
        echo "Building pangenome graph with Docker..." > {log}
        
        # Create relative paths for Docker
        REL_SEQS=$(realpath --relative-to={params.mount_dir} {input.seqs})
        REL_VG=$(realpath --relative-to={params.mount_dir} {output.vg})
        REL_GFA=$(realpath --relative-to={params.mount_dir} {output.gfa})
        
        # Run vg msga in Docker
        docker run --rm \
            -v {params.mount_dir}:/data \
            -w /data \
            {VG_DOCKER} \
            vg msga -f $REL_SEQS -k 32 -B 128 -t {threads} > $REL_VG 2>> {log}
        
        # Convert to GFA
        echo "Converting to GFA..." >> {log}
        docker run --rm \
            -v {params.mount_dir}:/data \
            -w /data \
            {VG_DOCKER} \
            vg view -g $REL_VG > $REL_GFA 2>> {log}
        """

# Alternative: Build using reference + single-sample VCFs
# This rule is likely not needed for the pangenome graph, but kept for now.
rule build_ref_graph:
    input:
        ref=REFERENCE
    output:
        vg=OUTPUT_DIR / "ref_graph.vg"
    log:
        OUTPUT_DIR / "logs" / "ref_graph.log"
    params:
        mount_dir=str(OUTPUT_DIR.parent.parent.absolute())
    shell:
        """
        REL_REF=$(realpath --relative-to={params.mount_dir} {input.ref})
        REL_VG=$(realpath --relative-to={params.mount_dir} {output.vg})
        
        docker run --rm \
            -v {params.mount_dir}:/data \
            -w /data \
            {VG_DOCKER} \
            vg construct -r $REL_REF -m 32 > $REL_VG 2> {log}
        """

# Call variants for each sample individually
# This rule is likely not needed for the pangenome graph, but kept for now.
rule align_and_call_sample:
    input:
        ref=REFERENCE,
        sample=HAPLOTYPE_DATA_DIR / "{sample_id}" / "{sample_id}_hap1.fasta" # Assuming hap1 for alignment
    output:
        bam=OUTPUT_DIR / "alignments" / "{sample_id}.bam",
        vcf=OUTPUT_DIR / "vcf_singles" / "{sample_id}.vcf"
    threads: 2
    log:
        OUTPUT_DIR / "logs" / "call_{sample_id}.log"
    shell:
        """
        mkdir -p $(dirname {output.bam}) $(dirname {output.vcf})
        
        # Align with minimap2
        minimap2 -a -x asm20 --cs {input.ref} {input.sample} 2> {log} | \
        samtools sort -o {output.bam} 2>> {log}
        samtools index {output.bam}
        
        # Call variants
        bcftools mpileup -Ou -f {input.ref} {output.bam} 2>> {log} | \
        bcftools call -mv -Ov -o {output.vcf} 2>> {log}
        """

# Build graph with each single-sample VCF
# This rule is likely not needed for the pangenome graph, but kept for now.
rule build_sample_graph:
    input:
        ref_graph=OUTPUT_DIR / "ref_graph.vg",
        vcf=OUTPUT_DIR / "vcf_singles" / "{sample}.vcf"
    output:
        vg=OUTPUT_DIR / "sample_graphs" / "{sample}.vg"
    log:
        OUTPUT_DIR / "logs" / "graph_{sample}.log"
    params:
        mount_dir=str(OUTPUT_DIR.parent.parent.absolute())
    shell:
        """
        mkdir -p $(dirname {output.vg})
        
        REL_GRAPH=$(realpath --relative-to={params.mount_dir} {input.ref_graph})
        REL_VCF=$(realpath --relative-to={params.mount_dir} {input.vcf})
        REL_OUT=$(realpath --relative-to={params.mount_dir} {output.vg})
        
        # Check if VCF has variants
        if grep -v '^#' {input.vcf} | head -1 | grep -q .; then
            docker run --rm \
                -v {params.mount_dir}:/data \
                -w /data \
                {VG_DOCKER} \
                vg mod -v $REL_VCF $REL_GRAPH > $REL_OUT 2> {log}
        else
            cp {input.ref_graph} $REL_OUT
            echo "No variants for {wildcards.sample}" >> {log}
        fi
        """

# Generate statistics
rule graph_stats:
    input:
        vg=OUTPUT_DIR / "pangenome.vg",
        gfa=OUTPUT_DIR / "pangenome.gfa",
        paf=OUTPUT_DIR / "sample_alignments.paf"
    output:
        stats=OUTPUT_DIR / "pangenome_stats.txt"
    params:
        mount_dir=str(OUTPUT_DIR.parent.parent.absolute())
    shell:
        """
        echo "=== Pangenome Statistics ===" > {output.stats}
        echo "" >> {output.stats}
        
        echo "Graph structure:" >> {output.stats}
        REL_VG=$(realpath --relative-to={params.mount_dir} {input.vg})
        docker run --rm \
            -v {params.mount_dir}:/data \
            -w /data \
            {VG_DOCKER} \
            vg stats -z $REL_VG >> {output.stats}
        
        echo -e "\nPaths in graph:" >> {output.stats}
        docker run --rm \
            -v {params.mount_dir}:/data \
            -w /data \
            {VG_DOCKER} \
            vg paths -v $REL_VG -L >> {output.stats} 2>/dev/null || echo "No paths found" >> {output.stats}
        
        echo -e "\nGFA summary:" >> {output.stats}
        echo -n "Nodes: " >> {output.stats}
        grep -c '^S' {input.gfa} >> {output.stats} || echo "0" >> {output.stats}
        echo -n "Edges: " >> {output.stats}
        grep -c '^L' {input.gfa} >> {output.stats} || echo "0" >> {output.stats}
        echo -n "Paths: " >> {output.stats}
        grep -c '^P' {input.gfa} >> {output.stats} || echo "0" >> {output.stats}
        
        echo -e "\nAlignment summary:" >> {output.stats}
        echo -n "Total alignments: " >> {output.stats}
        wc -l < {input.paf} >> {output.stats}
        """
