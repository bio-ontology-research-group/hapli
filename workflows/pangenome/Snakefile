import os
from pathlib import Path

# Load config - all paths and parameters will come from config
configfile: "config.yaml"

# All paths from config
REFERENCE = config["reference"]
HAPLOTYPE_FASTAS = config["haplotype_fastas"]
OUTPUT_DIR = config["output_dir"]
CACTUS_DOCKER = config.get("cactus_docker", "quay.io/comparative-genomics-toolkit/cactus:v2.9.8")
VG_DOCKER = config.get("vg_docker", "quay.io/vgteam/vg:v1.65.0")
THREADS = config.get("threads", 4)
MOUNT_DIR = config["mount_dir"]

# Get current user ID and group ID for Docker
USER_ID = os.getuid()
GROUP_ID = os.getgid()

# Create output directory
Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)

# Final outputs
rule all:
    input:
        os.path.join(OUTPUT_DIR, "pangenome.gfa"),
        os.path.join(OUTPUT_DIR, "pangenome_stats.txt"),
        os.path.join(OUTPUT_DIR, "sample_alignments.paf")

# Create seqFile for Cactus
rule create_seqfile:
    input:
        ref=REFERENCE,
        haplotypes=HAPLOTYPE_FASTAS
    output:
        seqfile=os.path.join(OUTPUT_DIR, "seqFile.txt"),
        combined=os.path.join(OUTPUT_DIR, "all_sequences.fa")
    log:
        os.path.join(OUTPUT_DIR, "logs", "create_seqfile.log")
    params:
        mount_dir=MOUNT_DIR
    run:
        # Create logs directory
        Path(str(log)).parent.mkdir(parents=True, exist_ok=True)
        
        # Create seqFile for Cactus (tab-separated: name<tab>path)
        # Paths must be relative to Docker mount point (/data)
        with open(str(output.seqfile), 'w') as seqfile:
            # Add reference - use first chromosome as reference
            ref_name = None
            with open(str(input.ref)) as f:
                for line in f:
                    if line.startswith('>'):
                        ref_name = line[1:].split()[0]
                        break
            
            if not ref_name:
                raise ValueError("Could not find reference chromosome name")
            
            # Convert absolute path to Docker container path
            ref_docker_path = "/data/" + os.path.relpath(str(input.ref), params.mount_dir)
            seqfile.write(f"{ref_name}\t{ref_docker_path}\n")
            print(f"Reference: {ref_name} -> {ref_docker_path} (host: {input.ref})", file=open(str(log), 'w'))
            
            # Add each haplotype with unique names to avoid duplicates
            sorted_haplotypes = sorted(input.haplotypes)
            for var_path in sorted_haplotypes:
                haplotype_name = Path(var_path).stem
                # Convert absolute path to Docker container path
                hap_docker_path = "/data/" + os.path.relpath(var_path, params.mount_dir)
                seqfile.write(f"{haplotype_name}\t{hap_docker_path}\n")
                print(f"Haplotype: {haplotype_name} -> {hap_docker_path} (host: {var_path})", file=open(str(log), 'a'))
        
        # Also create combined FASTA for alignment stats
        with open(str(output.combined), 'w') as out:
            # Add reference sequences, preserving original chromosome names
            print(f"Creating combined FASTA...", file=open(str(log), 'a'))
            with open(str(input.ref)) as f:
                for line in f:
                    if line.startswith('>'):
                        # Extract chromosome name from header
                        chrom_name = line[1:].split()[0]
                        out.write(f'>{chrom_name}\n')
                    else:
                        out.write(line)
            
            # Add each haplotype
            for var_path in sorted_haplotypes:
                haplotype_name = Path(var_path).stem 
                with open(var_path) as f:
                    for line in f:
                        if line.startswith('>'):
                            # Extract chromosome name and append haplotype suffix
                            chrom_name = line[1:].split()[0]
                            out.write(f'>{chrom_name}#{haplotype_name}\n')
                        else:
                            out.write(line)

# Create all-vs-all alignment for stats
rule align_all:
    input:
        seqs=os.path.join(OUTPUT_DIR, "all_sequences.fa")
    output:
        paf=os.path.join(OUTPUT_DIR, "sample_alignments.paf")
    threads: THREADS
    log:
        os.path.join(OUTPUT_DIR, "logs", "align_all.log")
    shell:
        """
        mkdir -p $(dirname {log})
        minimap2 -x asm20 -X -c -t {threads} {input.seqs} {input.seqs} > {output.paf} 2> {log}
        echo "Alignment complete. Lines in PAF:" >> {log}
        wc -l {output.paf} >> {log}
        """

# Build pangenome using Cactus
rule build_pangenome:
    input:
        seqfile=os.path.join(OUTPUT_DIR, "seqFile.txt")
    output:
        gfa=os.path.join(OUTPUT_DIR, "pangenome.gfa"),
        vg=os.path.join(OUTPUT_DIR, "pangenome.vg")
    threads: THREADS
    log:
        os.path.join(OUTPUT_DIR, "logs", "build_pangenome.log")
    params:
        mount_dir=MOUNT_DIR,
        cactus_docker=CACTUS_DOCKER,
        vg_docker=VG_DOCKER,
        user_id=USER_ID,
        group_id=GROUP_ID,
        output_dir=OUTPUT_DIR
    shell:
        """
        mkdir -p $(dirname {log})
        
        # Build pangenome using Cactus
        echo "Building pangenome graph with Cactus..." > {log}
        echo "Running as user {params.user_id}:{params.group_id}" >> {log}
        
        # Create relative paths for Docker (relative to mount point)
        REL_SEQFILE=$(realpath --relative-to={params.mount_dir} {input.seqfile})
        REL_OUTPUT_DIR=$(realpath --relative-to={params.mount_dir} {params.output_dir})
        
        echo "Mount dir: {params.mount_dir}" >> {log}
        echo "SeqFile: {input.seqfile} -> $REL_SEQFILE" >> {log}
        echo "Output dir: {params.output_dir} -> $REL_OUTPUT_DIR" >> {log}
        
        # Show seqFile contents for debugging
        echo "SeqFile contents:" >> {log}
        cat {input.seqfile} >> {log}
        
        # Get reference name from seqFile (first entry)
        REF_NAME=$(head -n1 {input.seqfile} | cut -f1)
        echo "Reference name: $REF_NAME" >> {log}
        
        # Create Cactus work directory and toil home
        CACTUS_WORK_DIR="$REL_OUTPUT_DIR/cactus_work"
        TOIL_HOME_DIR="$REL_OUTPUT_DIR/toil_home"
        
        # Clean up any existing work directories to avoid job store conflicts
        echo "Cleaning up existing work directories..." >> {log}
        rm -rf {params.output_dir}/cactus_work {params.output_dir}/toil_home {params.output_dir}/tmp {params.output_dir}/cactus_output
        
        # Pre-create directories with proper permissions
        mkdir -p {params.output_dir}/toil_home {params.output_dir}/tmp {params.output_dir}/cactus_output
        chmod 755 {params.output_dir}/toil_home {params.output_dir}/tmp {params.output_dir}/cactus_output
        
        # Clean up any existing job store using toil clean (in case of previous failed runs)
        echo "Cleaning up any existing job stores..." >> {log}
        docker run --rm \
            --user {params.user_id}:{params.group_id} \
            -v {params.mount_dir}:/data \
            -w /data \
            -e HOME="/data/$TOIL_HOME_DIR" \
            -e TOIL_HOME="/data/$TOIL_HOME_DIR" \
            -e TMPDIR="/data/$REL_OUTPUT_DIR/tmp" \
            -e XDG_RUNTIME_DIR="/data/$REL_OUTPUT_DIR/tmp" \
            {params.cactus_docker} \
            bash -c "
            export HOME=/data/$TOIL_HOME_DIR
            export TOIL_HOME=/data/$TOIL_HOME_DIR
            export TMPDIR=/data/$REL_OUTPUT_DIR/tmp
            export XDG_RUNTIME_DIR=/data/$REL_OUTPUT_DIR/tmp
            mkdir -p \$HOME \$TOIL_HOME \$TMPDIR \$XDG_RUNTIME_DIR
            if [ -d '/data/$CACTUS_WORK_DIR' ]; then
                echo 'Cleaning existing job store...'
                toil clean file:/data/$CACTUS_WORK_DIR || true
                rm -rf /data/$CACTUS_WORK_DIR || true
            fi
            " 2>> {log}
        
        # Run Cactus pangenome construction with increased memory and disk resources
        echo "Running cactus-pangenome..." >> {log}
        docker run --rm \
            --user {params.user_id}:{params.group_id} \
            -v {params.mount_dir}:/data \
            -w /data \
            -e HOME="/data/$TOIL_HOME_DIR" \
            -e TOIL_HOME="/data/$TOIL_HOME_DIR" \
            -e TMPDIR="/data/$REL_OUTPUT_DIR/tmp" \
            -e XDG_RUNTIME_DIR="/data/$REL_OUTPUT_DIR/tmp" \
            --memory=32g \
            --shm-size=8g \
            {params.cactus_docker} \
            bash -c "
            export HOME=/data/$TOIL_HOME_DIR
            export TOIL_HOME=/data/$TOIL_HOME_DIR
            export TMPDIR=/data/$REL_OUTPUT_DIR/tmp
            export XDG_RUNTIME_DIR=/data/$REL_OUTPUT_DIR/tmp
            mkdir -p \$HOME \$TOIL_HOME \$TMPDIR \$XDG_RUNTIME_DIR
            cactus-pangenome \
                /data/$CACTUS_WORK_DIR \
                /data/$REL_SEQFILE \
                --outDir /data/$REL_OUTPUT_DIR/cactus_output \
                --outName pangenome \
                --reference $REF_NAME \
                --gfa \
                --vcf \
                --gbz \
                --maxCores {threads} \
                --maxMemory 32Gi \
                --maxDisk 1000Gi \
                --logLevel INFO
            " 2>> {log}
        
        # Check if Cactus output was created (handle both compressed and uncompressed GFA)
        if [ -f {params.output_dir}/cactus_output/pangenome.gfa.gz ]; then
            echo "Found compressed GFA file, decompressing..." >> {log}
            gunzip -c {params.output_dir}/cactus_output/pangenome.gfa.gz > {output.gfa}
        elif [ -f {params.output_dir}/cactus_output/pangenome.gfa ]; then
            echo "Found uncompressed GFA file, copying..." >> {log}
            cp {params.output_dir}/cactus_output/pangenome.gfa {output.gfa}
        else
            echo "ERROR: No GFA file found (compressed or uncompressed)" >> {log}
            echo "Contents of cactus_output directory:" >> {log}
            ls -la {params.output_dir}/cactus_output/ >> {log} 2>&1 || echo "cactus_output directory does not exist" >> {log}
            cat {log}
            exit 1
        fi
        
        # Convert GFA to VG format using vg (use -p instead of deprecated -v)
        echo "Converting GFA to VG format..." >> {log}
        REL_GFA=$(realpath --relative-to={params.mount_dir} {output.gfa})
        REL_VG=$(realpath --relative-to={params.mount_dir} {output.vg})
        
        echo "GFA path for Docker: /data/$REL_GFA" >> {log}
        echo "VG path for Docker: /data/$REL_VG" >> {log}
        
        docker run --rm \
            --user {params.user_id}:{params.group_id} \
            -v {params.mount_dir}:/data \
            -w /data \
            {params.vg_docker} \
            bash -c "vg convert -g '/data/$REL_GFA' -p > '/data/$REL_VG'" 2>> {log}
        
        # Check if VG file was created
        if [ ! -f {output.vg} ]; then
            echo "ERROR: VG file was not created" >> {log}
            echo "Expected VG file: {output.vg}" >> {log}
            echo "Docker VG path: /data/$REL_VG" >> {log}
            cat {log}
            exit 1
        fi
        
        echo "Pangenome construction completed successfully" >> {log}
        """

# Generate statistics
rule graph_stats:
    input:
        vg=os.path.join(OUTPUT_DIR, "pangenome.vg"),
        gfa=os.path.join(OUTPUT_DIR, "pangenome.gfa"),
        paf=os.path.join(OUTPUT_DIR, "sample_alignments.paf")
    output:
        stats=os.path.join(OUTPUT_DIR, "pangenome_stats.txt")
    params:
        mount_dir=MOUNT_DIR,
        vg_docker=VG_DOCKER,
        user_id=USER_ID,
        group_id=GROUP_ID
    shell:
        """
        echo "=== Pangenome Statistics ===" > {output.stats}
        echo "" >> {output.stats}
        
        echo "Graph structure:" >> {output.stats}
        REL_VG=$(realpath --relative-to={params.mount_dir} {input.vg})
        docker run --rm \
            --user {params.user_id}:{params.group_id} \
            -v {params.mount_dir}:/data \
            -w /data \
            {params.vg_docker} \
            vg stats -z "/data/$REL_VG" >> {output.stats}
        
        echo -e "\nPaths in graph:" >> {output.stats}
        docker run --rm \
            --user {params.user_id}:{params.group_id} \
            -v {params.mount_dir}:/data \
            -w /data \
            {params.vg_docker} \
            vg paths -v "/data/$REL_VG" -L >> {output.stats} 2>/dev/null || echo "No paths found" >> {output.stats}
        
        echo -e "\nGFA summary:" >> {output.stats}
        echo -n "Nodes: " >> {output.stats}
        grep -c '^S' {input.gfa} >> {output.stats} || echo "0" >> {output.stats}
        echo -n "Edges: " >> {output.stats}
        grep -c '^L' {input.gfa} >> {output.stats} || echo "0" >> {output.stats}
        echo -n "Paths: " >> {output.stats}
        grep -c '^P' {input.gfa} >> {output.stats} || echo "0" >> {output.stats}
        
        echo -e "\nAlignment summary:" >> {output.stats}
        echo -n "Total alignments: " >> {output.stats}
        wc -l < {input.paf} >> {output.stats}
        """
